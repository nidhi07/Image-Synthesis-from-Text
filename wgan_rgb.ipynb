{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, merge\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.backend as K\n",
    " \n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imread, imsave\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN():\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 3\n",
    "        self.img_shape = (1,self.img_rows*self.img_cols*self.channels)\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        self.clip_value = 0.01\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "       \n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(4596,))\n",
    "        img = self.generator(z)\n",
    "        \n",
    "        img_merge = concatenate([img,z], axis=1)\n",
    "        print (\"img_merge\")\n",
    "        print (img_merge)\n",
    "        print (img_merge.shape)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img_merge)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=self.wasserstein_loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        noise_shape = (4596,)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_shape=noise_shape))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "        \n",
    "        #model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        #img = Concatenate([img,noise])\n",
    "        \n",
    "        return Model(noise, img)\n",
    " \n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        #img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        img_shape = (self.img_rows*self.img_cols*self.channels+4596,)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        features = model(img)\n",
    "        valid = Dense(1, activation=\"linear\")(features)\n",
    "\n",
    "        return Model(img, valid)\n",
    "\n",
    "    def get_image(self, image_path, width, height, mode):\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        # image = image.resize([width, height], Image.BILINEAR)\n",
    "        if image.size != (width, height):\n",
    "        # Remove most pixels that aren't part of a face\n",
    "            face_width = face_height = 108\n",
    "            j = (image.size[0] - face_width) // 2\n",
    "            i = (image.size[1] - face_height) // 2\n",
    "            image = image.crop([j, i, j + face_width, i + face_height])\n",
    "            image = image.resize([width, height])\n",
    "\n",
    "        return np.array(image.convert(mode))\n",
    "\n",
    "    def get_batch(self, image_files, width, height, mode):\n",
    "        data_batch = np.array(\n",
    "            [self.get_image(sample_file, width, height, mode) for sample_file in image_files])\n",
    "\n",
    "        return data_batch\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        X_train_green_fr = self.get_batch(glob(os.path.join('data_final/green_fruit', '*'))[:490], 28, 28, 'RGB')\n",
    "        X_train_purple_fr = self.get_batch(glob(os.path.join('data_final/purple_fruit', '*'))[:490], 28, 28, 'RGB')\n",
    "        X_train_red_fl = self.get_batch(glob(os.path.join('data_final/red_flower', '*'))[:322], 28, 28, 'RGB')\n",
    "        X_train_yellow_fl = self.get_batch(glob(os.path.join('data_final/yellow_flower', '*'))[:490], 28, 28, 'RGB')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        y_train_green_fr = np.genfromtxt('embeddings/greenpear.csv',delimiter=\",\").reshape(1,4096);\n",
    "        y_train_purple_fr = np.genfromtxt('embeddings/purplepear.csv',delimiter=\",\").reshape(1,4096); \n",
    "        y_train_red_fl = np.genfromtxt('embeddings/redbanana.csv',delimiter=\",\").reshape(1,4096);\n",
    "        y_train_yellow_fl = np.genfromtxt('embeddings/yellowbanana.csv',delimiter=\",\").reshape(1,4096); \n",
    "        \n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(X_train_blue_fl.shape)\n",
    "        # Rescale -1 to 1\n",
    "        X_train_green_fr = (X_train_green_fr.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train_purple_fr = (X_train_purple_fr.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train_red_fl = (X_train_red_fl.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train_yellow_fl = (X_train_yellow_fl.astype(np.float32) - 127.5) / 127.5\n",
    "        # Adversarial ground truth\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            \n",
    "            for _ in range(self.n_critic):\n",
    "            \n",
    "            \n",
    "            \n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                G_repeated = np.asarray([y_train_green_fr,]*int(batch_size/4))\n",
    "                P_repeated = np.asarray([y_train_purple_fr,]*int(batch_size/4))\n",
    "                R_repeated = np.asarray([y_train_red_fl,]*int(batch_size/4))\n",
    "                Y_repeated = np.asarray([y_train_yellow_fl,]*int(batch_size/4))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "                # Sample a random batch of images\n",
    "                idx = np.random.randint(0, X_train_green_fr.shape[0], int(batch_size/4))\n",
    "                imgs_green_fr = X_train_green_fr[idx]\n",
    "            \n",
    "                #appending with embedding blue flowers\n",
    "                print (imgs_green_fr.shape)\n",
    "                print (G_repeated.shape)\n",
    "            \n",
    "                #reshaping\n",
    "                imgs_green_fr = imgs_green_fr.reshape(int(batch_size/4),28*28*3)\n",
    "                G_repeated = G_repeated.reshape(int(batch_size/4),4096)\n",
    "\n",
    "                imgs_green_fr_with_emb = np.concatenate((imgs_green_fr, G_repeated),axis=1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                # Sample a random batch of images\n",
    "                idx = np.random.randint(0, X_train_purple_fr.shape[0], int(batch_size/4))\n",
    "                imgs_purple_fr = X_train_purple_fr[idx]\n",
    "            \n",
    "                #reshaping\n",
    "                imgs_purple_fr = imgs_purple_fr.reshape(int(batch_size/4),28*28*3)\n",
    "                P_repeated = P_repeated.reshape(int(batch_size/4),4096)\n",
    "            \n",
    "                #appending with embedding purple fruit\n",
    "                imgs_purple_fr_with_emb = np.concatenate((imgs_purple_fr, P_repeated),axis=1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                # Sample a random batch of images\n",
    "                idx = np.random.randint(0, X_train_red_fl.shape[0], int(batch_size/4))\n",
    "                imgs_red_fl = X_train_red_fl[idx]\n",
    "            \n",
    "                #appending with embedding blue flowers\n",
    "            \n",
    "                #reshaping\n",
    "                imgs_red_fl = imgs_red_fl.reshape(int(batch_size/4),28*28*3)\n",
    "                R_repeated = R_repeated.reshape(int(batch_size/4),4096)\n",
    "\n",
    "                imgs_red_fl_with_emb = np.concatenate((imgs_red_fl, R_repeated),axis=1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                # Sample a random batch of images\n",
    "                idx = np.random.randint(0, X_train_yellow_fl.shape[0], int(batch_size/4))\n",
    "                imgs_yellow_fl = X_train_yellow_fl[idx]\n",
    "            \n",
    "                #reshaping\n",
    "                imgs_yellow_fl = imgs_yellow_fl.reshape(int(batch_size/4),28*28*3)\n",
    "                Y_repeated = Y_repeated.reshape(int(batch_size/4),4096)\n",
    "            \n",
    "                #appending with embedding yellow fruit\n",
    "                imgs_yellow_fl_with_emb = np.concatenate((imgs_yellow_fl, Y_repeated),axis=1)\n",
    "            \n",
    "            \n",
    "            \n",
    "                imgs = np.concatenate((imgs_green_fr_with_emb, imgs_purple_fr_with_emb),axis=0)\n",
    "                imgs = np.concatenate((imgs, imgs_red_fl_with_emb),axis=0)\n",
    "                imgs = np.concatenate((imgs, imgs_yellow_fl_with_emb),axis=0)\n",
    "            \n",
    "            \n",
    "                #Generating noise\n",
    "                noise = np.random.normal(0, 1, (batch_size, 500))\n",
    "                noise = noise.reshape(batch_size,500)\n",
    "            \n",
    "            \n",
    "                imgs = np.concatenate((imgs, noise),axis=1)\n",
    "            \n",
    "            \n",
    "                # Sample generator input\n",
    "                #noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "            \n",
    "            \n",
    "                #Generating noise\n",
    "                noise = np.random.normal(0, 1, (batch_size, 500))\n",
    "                noise = noise.reshape(batch_size,500)\n",
    "\n",
    "                text_embed = np.concatenate((G_repeated,P_repeated),axis=0)\n",
    "                text_embed = np.concatenate((text_embed,R_repeated),axis=0)\n",
    "                text_embed = np.concatenate((text_embed,Y_repeated),axis=0)  \n",
    "            \n",
    "            \n",
    "                text_embed = np.concatenate((text_embed, noise),axis=1)\n",
    "            \n",
    "            \n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(text_embed)\n",
    "            \n",
    "            \n",
    "                print (gen_imgs.shape)\n",
    "                print (text_embed.shape)\n",
    "            \n",
    "                gen_imgs = gen_imgs.reshape(batch_size, 28*28*3)\n",
    "                print (text_embed.shape)\n",
    "            \n",
    "                #concatenate embeddings wit the generated images\n",
    "                gen_imgs = np.concatenate((gen_imgs, text_embed),axis=1)\n",
    "            \n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, -np.ones((batch_size, 1)))\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.ones((batch_size, 1)))\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "                \n",
    "                # Clip discriminator weights\n",
    "                for l in self.discriminator.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Sample generator input\n",
    "            #noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(text_embed, -np.ones((batch_size, 1)))\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, 1-d_loss[0], 100*d_loss[1], 1-g_loss[0]))\n",
    "            file = open (\"128_batch\",\"w+\")\n",
    "            file.write (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, 1-d_loss[0], 100*d_loss[1], 1-g_loss[0]))\n",
    "            \n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch, text_embed)\n",
    "                \n",
    "            \n",
    "\n",
    "    def save_imgs(self, epoch, text_embed):\n",
    "        r, c = 4, 8\n",
    "        #noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_imgs = self.generator.predict(text_embed)\n",
    "        \n",
    "        #dimensions\n",
    "        print(gen_imgs.shape)\n",
    "        gen_imgs = gen_imgs.reshape(32,28,28,3) #hardcoded batch-size 32\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = (1/2.5) * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,:])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"WGAN_output_banana_pear/%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 512)               3557888   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,689,473\n",
      "Trainable params: 3,689,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 256)               1176832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 2352)              2410800   \n",
      "=================================================================\n",
      "Total params: 4,251,696\n",
      "Trainable params: 4,248,112\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "img_merge\n",
      "Tensor(\"concatenate_5/concat:0\", shape=(?, 6948), dtype=float32)\n",
      "(?, 6948)\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.966238, acc.: 45.31%] [G loss: 1.520115]\n",
      "(32, 2352)\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "1 [D loss: 1.196330, acc.: 25.00%] [G loss: 1.440716]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "2 [D loss: 1.232466, acc.: 20.31%] [G loss: 1.389605]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "3 [D loss: 1.268457, acc.: 12.50%] [G loss: 1.351772]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "4 [D loss: 1.289591, acc.: 3.12%] [G loss: 1.313580]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "5 [D loss: 1.298710, acc.: 1.56%] [G loss: 1.290181]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "6 [D loss: 1.312957, acc.: 4.69%] [G loss: 1.279313]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "7 [D loss: 1.334416, acc.: 0.00%] [G loss: 1.215967]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "8 [D loss: 1.342258, acc.: 0.00%] [G loss: 1.228833]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "9 [D loss: 1.366623, acc.: 0.00%] [G loss: 1.187233]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "10 [D loss: 1.385680, acc.: 0.00%] [G loss: 1.172235]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "11 [D loss: 1.384964, acc.: 0.00%] [G loss: 1.153812]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "12 [D loss: 1.400531, acc.: 0.00%] [G loss: 1.123931]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "13 [D loss: 1.413787, acc.: 0.00%] [G loss: 1.119356]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "14 [D loss: 1.426408, acc.: 0.00%] [G loss: 1.105997]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "15 [D loss: 1.429206, acc.: 0.00%] [G loss: 1.096382]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "16 [D loss: 1.438965, acc.: 0.00%] [G loss: 1.081113]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "17 [D loss: 1.447475, acc.: 0.00%] [G loss: 1.079294]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "18 [D loss: 1.447581, acc.: 0.00%] [G loss: 1.071566]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "19 [D loss: 1.458105, acc.: 0.00%] [G loss: 1.060559]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "20 [D loss: 1.460645, acc.: 0.00%] [G loss: 1.055864]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "21 [D loss: 1.467482, acc.: 0.00%] [G loss: 1.048218]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "22 [D loss: 1.468577, acc.: 0.00%] [G loss: 1.049804]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "23 [D loss: 1.465820, acc.: 0.00%] [G loss: 1.036523]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "24 [D loss: 1.478162, acc.: 0.00%] [G loss: 1.032867]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "25 [D loss: 1.481243, acc.: 0.00%] [G loss: 1.027234]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "26 [D loss: 1.483950, acc.: 0.00%] [G loss: 1.026003]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "27 [D loss: 1.483477, acc.: 0.00%] [G loss: 1.025239]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "28 [D loss: 1.484208, acc.: 0.00%] [G loss: 1.023443]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "29 [D loss: 1.483017, acc.: 0.00%] [G loss: 1.020128]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "30 [D loss: 1.488420, acc.: 0.00%] [G loss: 1.017742]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "31 [D loss: 1.489644, acc.: 0.00%] [G loss: 1.015652]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "32 [D loss: 1.488957, acc.: 0.00%] [G loss: 1.013957]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "33 [D loss: 1.490146, acc.: 0.00%] [G loss: 1.012450]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "34 [D loss: 1.491852, acc.: 0.00%] [G loss: 1.013215]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "35 [D loss: 1.492713, acc.: 0.00%] [G loss: 1.009291]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "36 [D loss: 1.493346, acc.: 0.00%] [G loss: 1.008610]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "37 [D loss: 1.493831, acc.: 0.00%] [G loss: 1.008179]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "38 [D loss: 1.495838, acc.: 0.00%] [G loss: 1.007110]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "39 [D loss: 1.495174, acc.: 0.00%] [G loss: 1.006718]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "40 [D loss: 1.494901, acc.: 0.00%] [G loss: 1.005900]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "41 [D loss: 1.495659, acc.: 0.00%] [G loss: 1.005104]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "42 [D loss: 1.496562, acc.: 0.00%] [G loss: 1.004898]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "43 [D loss: 1.497622, acc.: 0.00%] [G loss: 1.004567]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "44 [D loss: 1.497302, acc.: 0.00%] [G loss: 1.004179]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "45 [D loss: 1.497690, acc.: 0.00%] [G loss: 1.003679]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "46 [D loss: 1.497170, acc.: 0.00%] [G loss: 1.004100]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "47 [D loss: 1.497819, acc.: 0.00%] [G loss: 1.002760]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "48 [D loss: 1.498059, acc.: 0.00%] [G loss: 1.002311]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "49 [D loss: 1.498307, acc.: 0.00%] [G loss: 1.002579]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "50 [D loss: 1.498444, acc.: 0.00%] [G loss: 1.001945]\n",
      "(32, 2352)\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "51 [D loss: 1.498804, acc.: 0.00%] [G loss: 1.001869]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "52 [D loss: 1.498828, acc.: 0.00%] [G loss: 1.001763]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "53 [D loss: 1.498786, acc.: 0.00%] [G loss: 1.001469]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "54 [D loss: 1.499031, acc.: 0.00%] [G loss: 1.001344]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "55 [D loss: 1.499263, acc.: 0.00%] [G loss: 1.001313]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "56 [D loss: 1.499228, acc.: 0.00%] [G loss: 1.001078]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "57 [D loss: 1.499386, acc.: 0.00%] [G loss: 1.000960]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "58 [D loss: 1.499291, acc.: 0.00%] [G loss: 1.000892]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "59 [D loss: 1.499496, acc.: 0.00%] [G loss: 1.000769]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "60 [D loss: 1.499462, acc.: 0.00%] [G loss: 1.000775]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "61 [D loss: 1.499523, acc.: 0.00%] [G loss: 1.000629]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "62 [D loss: 1.499633, acc.: 0.00%] [G loss: 1.000566]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "63 [D loss: 1.499640, acc.: 0.00%] [G loss: 1.000537]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "64 [D loss: 1.499675, acc.: 0.00%] [G loss: 1.000505]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "65 [D loss: 1.499706, acc.: 0.00%] [G loss: 1.000405]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "66 [D loss: 1.499769, acc.: 0.00%] [G loss: 1.000359]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "67 [D loss: 1.499740, acc.: 0.00%] [G loss: 1.000399]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "68 [D loss: 1.499762, acc.: 0.00%] [G loss: 1.000315]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "69 [D loss: 1.499776, acc.: 0.00%] [G loss: 1.000284]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "70 [D loss: 1.499861, acc.: 0.00%] [G loss: 1.000239]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "71 [D loss: 1.499789, acc.: 0.00%] [G loss: 1.000254]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 [D loss: 1.499873, acc.: 0.00%] [G loss: 1.000192]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "73 [D loss: 1.499860, acc.: 0.00%] [G loss: 1.000205]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "74 [D loss: 1.499899, acc.: 0.00%] [G loss: 1.000145]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "75 [D loss: 1.499875, acc.: 0.00%] [G loss: 1.000154]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "76 [D loss: 1.499915, acc.: 0.00%] [G loss: 1.000146]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "77 [D loss: 1.499914, acc.: 0.00%] [G loss: 1.000127]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "78 [D loss: 1.499936, acc.: 0.00%] [G loss: 1.000100]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "79 [D loss: 1.499930, acc.: 0.00%] [G loss: 1.000097]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "80 [D loss: 1.499940, acc.: 0.00%] [G loss: 1.000100]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "81 [D loss: 1.499952, acc.: 0.00%] [G loss: 1.000073]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "82 [D loss: 1.499945, acc.: 0.00%] [G loss: 1.000073]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "83 [D loss: 1.499958, acc.: 0.00%] [G loss: 1.000061]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "84 [D loss: 1.499959, acc.: 0.00%] [G loss: 1.000065]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "85 [D loss: 1.499957, acc.: 0.00%] [G loss: 1.000060]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "86 [D loss: 1.499972, acc.: 0.00%] [G loss: 1.000049]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "87 [D loss: 1.499971, acc.: 0.00%] [G loss: 1.000052]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "88 [D loss: 1.499972, acc.: 0.00%] [G loss: 1.000041]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "89 [D loss: 1.499975, acc.: 0.00%] [G loss: 1.000039]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "90 [D loss: 1.499978, acc.: 0.00%] [G loss: 1.000034]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "91 [D loss: 1.499975, acc.: 0.00%] [G loss: 1.000029]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "92 [D loss: 1.499981, acc.: 0.00%] [G loss: 1.000031]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "93 [D loss: 1.499985, acc.: 0.00%] [G loss: 1.000025]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "94 [D loss: 1.499984, acc.: 0.00%] [G loss: 1.000025]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "95 [D loss: 1.499984, acc.: 0.00%] [G loss: 1.000021]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "96 [D loss: 1.499986, acc.: 0.00%] [G loss: 1.000020]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "97 [D loss: 1.499988, acc.: 0.00%] [G loss: 1.000017]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "98 [D loss: 1.499989, acc.: 0.00%] [G loss: 1.000018]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "99 [D loss: 1.499990, acc.: 0.00%] [G loss: 1.000015]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "100 [D loss: 1.499990, acc.: 0.00%] [G loss: 1.000012]\n",
      "(32, 2352)\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "101 [D loss: 1.499992, acc.: 0.00%] [G loss: 1.000015]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "102 [D loss: 1.499992, acc.: 0.00%] [G loss: 1.000011]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "103 [D loss: 1.499994, acc.: 0.00%] [G loss: 1.000010]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "104 [D loss: 1.499993, acc.: 0.00%] [G loss: 1.000009]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "105 [D loss: 1.499994, acc.: 0.00%] [G loss: 1.000009]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "106 [D loss: 1.499995, acc.: 0.00%] [G loss: 1.000009]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "107 [D loss: 1.499996, acc.: 0.00%] [G loss: 1.000007]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "108 [D loss: 1.499996, acc.: 0.00%] [G loss: 1.000007]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "109 [D loss: 1.499996, acc.: 0.00%] [G loss: 1.000006]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "110 [D loss: 1.499996, acc.: 0.00%] [G loss: 1.000006]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "111 [D loss: 1.499997, acc.: 0.00%] [G loss: 1.000005]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "112 [D loss: 1.499996, acc.: 0.00%] [G loss: 1.000006]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "113 [D loss: 1.499997, acc.: 0.00%] [G loss: 1.000005]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "114 [D loss: 1.499997, acc.: 0.00%] [G loss: 1.000005]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "115 [D loss: 1.499997, acc.: 0.00%] [G loss: 1.000004]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "116 [D loss: 1.499998, acc.: 0.00%] [G loss: 1.000004]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "117 [D loss: 1.499998, acc.: 0.00%] [G loss: 1.000004]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "118 [D loss: 1.499998, acc.: 0.00%] [G loss: 1.000004]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "119 [D loss: 1.499998, acc.: 0.00%] [G loss: 1.000003]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "120 [D loss: 1.499998, acc.: 0.00%] [G loss: 1.000003]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "121 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000003]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "122 [D loss: 1.499998, acc.: 0.00%] [G loss: 1.000003]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "123 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000002]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "124 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000002]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "125 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000002]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "126 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000002]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "127 [D loss: 1.499998, acc.: 0.00%] [G loss: 1.000002]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "128 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000002]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "129 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000002]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "130 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "131 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "132 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000002]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "133 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "134 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "135 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "136 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "137 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "138 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "139 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "140 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "141 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "142 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "144 [D loss: 1.499999, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "145 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "146 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "147 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "148 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "149 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "150 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(32, 2352)\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "151 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "152 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "153 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "154 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "155 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "156 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000001]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "157 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "158 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "159 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "160 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "161 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "162 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "163 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "164 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "165 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "166 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "167 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "168 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "169 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "170 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "171 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "172 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "173 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "174 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "175 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "176 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "177 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "178 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "179 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "180 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "181 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "182 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "183 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "184 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "185 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "186 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "187 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "188 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "189 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "190 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "191 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "192 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "193 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "194 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "195 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "196 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "197 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "198 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "199 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "200 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(32, 2352)\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "201 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "202 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "203 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "204 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "205 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "206 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "207 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "208 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "209 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "210 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "211 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "212 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "213 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "215 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "216 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "217 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "218 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "219 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "220 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "221 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "222 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "223 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "224 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "225 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "226 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "227 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "228 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "229 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "230 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "231 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "232 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "233 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "234 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "235 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "236 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "237 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "238 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "239 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "240 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "241 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "242 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "243 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "244 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "245 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "246 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n",
      "247 [D loss: 1.500000, acc.: 0.00%] [G loss: 1.000000]\n",
      "(8, 28, 28, 3)\n",
      "(8, 1, 4096)\n",
      "(32, 2352)\n",
      "(32, 4596)\n",
      "(32, 4596)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-15ce563c3de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-703b3533a671>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=10000, batch_size=32, save_interval=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
